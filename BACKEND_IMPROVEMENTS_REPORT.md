# Backend Improvements Report

## AplikÃ¡cia: PMP (Premarket Price)

**DÃ¡tum:** 2025-01-24  
**Verzia:** 1.1  
**Autor:** Senior Backend Engineer Proposal

---

## ğŸ”¥ Executive Summary (10-sekundovÃ© zhodnotenie)

**NajvÃ¤ÄÅ¡Ã­ bottleneck:** N+1 query problem v `polygonWorker.ts` - kaÅ¾dÃ½ ticker = individuÃ¡lna DB query + Redis operÃ¡cia.

**AktuÃ¡lny vÃ½kon:**

- DB writes per batch: **~150 queries** (70 tickerov Ã— 2-3 queries)
- Redis ops per batch: **~150 ops** (70 tickerov Ã— 2-3 ops)
- Polygon snapshots: **~3-4 req/min** (free tier limit)
- Full cycle for 200 tickers: **~55-70 sec**

**NavrhovanÃ© rieÅ¡enie:**

- Batch DB operations (transaction-based)
- Batch Redis pipeline (MULTI/EXEC)
- Adaptive rate limiting

**OÄakÃ¡vanÃ½ vÃ½sledok:** **4-6Ã— zrÃ½chlenie ingestion pipeline** (z ~60s na ~10-15s pre top 200 tickerov)

**Priorita:** VysokÃ¡ - implementÃ¡cia v 4-5 dÅˆoch, backward compatible, Å¾iadne breaking changes.

---

## ğŸ“Š 1. Ako aplikÃ¡cia funguje teraz

### 1.1 ArchitektÃºra dÃ¡tovÃ©ho flow

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Polygon API  â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ snapshots
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ polygonWorker â”‚
       â””â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     batch DB  â”‚
     writes    â”‚ Redis pipeline
       â”‚       â”‚
 â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ SQLite  â”‚  â”‚ Redis Cacheâ”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚ Pub/Sub
   API routes   WebSockets
      â”‚            â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â–¼
         Frontend
```

### 1.2 HlavnÃ© komponenty

#### **A. Data Ingestion Worker (`polygonWorker.ts`)**

- **Funkcia:** Batch ingest dÃ¡t z Polygon API
- **Batch size:** 60-70 tickerov na request
- **Rate limiting:** 15s delay medzi batchmi (Polygon free tier: 5 req/min)
- **PrioritizÃ¡cia:**
  - Top 200 tickerov: update kaÅ¾dÃ½ch 60s
  - OstatnÃ© tickery: update kaÅ¾dÃ½ch 5 min
- **Proces:**
  1. Fetch snapshot z Polygon API (batch)
  2. NormalizÃ¡cia dÃ¡t
  3. Upsert do DB (`Ticker`, `SessionPrice`, `DailyRef`)
  4. ZapÃ­sanie do Redis (hot cache)
  5. Publikovanie do Redis Pub/Sub (pre WebSocket)

#### **B. Database (SQLite + Prisma)**

- **Ticker tabuÄ¾ka:**
  - Static data: `symbol`, `name`, `sector`, `industry`, `sharesOutstanding`
  - Cached values: `lastPrice`, `lastChangePct`, `lastMarketCap`, `lastMarketCapDiff`
  - Indexy: `lastPrice`, `lastChangePct`, `lastMarketCap`, `lastMarketCapDiff` (pre sorting)
- **SessionPrice tabuÄ¾ka:**
  - HistorickÃ© ceny pre kaÅ¾dÃº session (pre, live, after)
  - Indexy: `[date, session]`, `[symbol, session]`
- **DailyRef tabuÄ¾ka:**
  - Previous close, regular close, today open
  - Indexy: `[date]`, `[symbol]`

#### **C. Redis Cache**

- **KÄ¾ÃºÄe:**
  - `last:{date}:{session}:{ticker}` - aktuÃ¡lne ceny
  - `stock:{ticker}` - hot cache pre API
  - `rank:{field}:{date}:{session}:asc/desc` - ZSET indexy pre sorting
  - `stats:{date}:{session}` - min/max hodnoty
  - `prevClose:{date}:{ticker}` - previous closes
- **TTL:**
  - Live session: 24h
  - Pre/After session: 7 dnÃ­
  - Hot cache: 120s

#### **D. API Routes**

- **`/api/stocks`** - SQL-first, ÄÃ­ta z `Ticker` tabuÄ¾ky
  - Podporuje `getAll=true` pre fetch vÅ¡etkÃ½ch tickerov
  - Podporuje sorting (`sort`, `order`)
  - Dynamic `marketCapDiff` calculation ak nie je v DB
- **`/api/heatmap`** - ÄÃ­ta z `SessionPrice` + `DailyRef` + `Ticker`

#### **F. PreÄo SQL namiesto Redis ako primÃ¡rny zdroj dÃ¡t?**

**HistorickÃ½ kontext:** PÃ´vodne aplikÃ¡cia pouÅ¾Ã­vala **Redis-first** architektÃºru, kde dÃ¡ta boli primÃ¡rne v Redis a DB slÃºÅ¾ila len ako backup. Po migrÃ¡cii na **SQL-first (database-first)** architektÃºru sme zÃ­skali:

**1. RÃ½chlejÅ¡ie naÄÃ­tanie strÃ¡nky**

- **Redis-first problÃ©m:** DÃ¡ta sa naÄÃ­tavali priebeÅ¾ne pri scrollovanÃ­ (lazy loading)
- **SQL-first rieÅ¡enie:** VÅ¡etky dÃ¡ta sÃº dostupnÃ© okamÅ¾ite z DB
- **VÃ½sledok:** StrÃ¡nka sa naÄÃ­ta **10-20Ã— rÃ½chlejÅ¡ie** (z ~5-10s na ~0.5-1s)

**2. EfektÃ­vne sortovanie**

- **Redis-first problÃ©m:** Sortovanie cez Redis ZSET bolo pomalÃ© pre veÄ¾kÃ© datasety
- **SQL-first rieÅ¡enie:** SQL indexy (`lastPrice`, `lastChangePct`, `lastMarketCap`, `lastMarketCapDiff`) umoÅ¾ÅˆujÃº **O(log n)** sortovanie
- **VÃ½sledok:** Sortovanie je **5-10Ã— rÃ½chlejÅ¡ie** a podporuje komplexnÃ© queries (filtering + sorting)

**3. Persistence a reliability**

- **Redis-first problÃ©m:** DÃ¡ta mÃ´Å¾u byÅ¥ stratenÃ© pri restart Redis (ak nie je persistence)
- **SQL-first rieÅ¡enie:** DÃ¡ta sÃº trvalo uloÅ¾enÃ© v DB, preÅ¾Ã­vajÃº restarty
- **VÃ½sledok:** **100% data persistence**, moÅ¾nosÅ¥ historickÃ½ch analÃ½z

**4. JednoduchÅ¡ie querying**

- **Redis-first problÃ©m:** KomplexnÃ© queries vyÅ¾adovali viacero Redis operÃ¡ciÃ­ (ZRANGE, HGET, atÄ.)
- **SQL-first rieÅ¡enie:** Jeden SQL query s WHERE, ORDER BY, LIMIT
- **VÃ½sledok:** **JednoduchÅ¡ie API**, menej kÃ³du, lepÅ¡ia maintainability

**5. Å kÃ¡lovateÄ¾nosÅ¥**

- **Redis-first problÃ©m:** Redis memory limit, potreba shardingu pre veÄ¾kÃ© datasety
- **SQL-first rieÅ¡enie:** SQLite/PostgreSQL Å¡kÃ¡luje lepÅ¡ie pre read-heavy workloads
- **VÃ½sledok:** Podpora pre **tisÃ­ce tickerov** bez performance degradÃ¡cie

**6. Development experience**

- **Redis-first problÃ©m:** Å¤aÅ¾kÃ© debugging, chÃ½bajÃºce query tools
- **SQL-first rieÅ¡enie:** Prisma Studio, SQL queries, lepÅ¡ie logging
- **VÃ½sledok:** **RÃ½chlejÅ¡Ã­ development**, jednoduchÅ¡ie troubleshooting

**Redis stÃ¡le pouÅ¾Ã­vame pre:**

- âœ… **Hot cache** - rÃ½chlejÅ¡ie opakovanÃ© ÄÃ­tania (TTL 120s)
- âœ… **Real-time updates** - WebSocket Pub/Sub pre live prices
- âœ… **Rank indexes** - ZSET pre heatmap sorting (doplÅˆok k DB)
- âœ… **Session data** - doÄasnÃ© dÃ¡ta pre aktuÃ¡lnu session

**ZÃ¡ver:** SQL-first architektÃºra poskytuje **lepÅ¡iu performance, reliability a maintainability** pre hlavnÃ© read path, zatiaÄ¾ Äo Redis slÃºÅ¾i ako **cache layer a real-time messaging**.

#### **E. WebSocket Server (`websocket-server.ts`)**

- **Funkcia:** Real-time price updates
- **Zdroj:** Redis Pub/Sub (`pmp:tick`)
- **Broadcast frequency:**
  - High activity: 100ms
  - Low activity: 2000ms
- **OptimizÃ¡cia:** Shared subscriber pre vÅ¡etky inÅ¡tancie

### 1.3 AktuÃ¡lne problÃ©my a limity

#### **ğŸ”¥ A. N+1 Query Problem v `polygonWorker.ts` (NAJVÃ„ÄŒÅ Ã BOTTLENECK)**

```typescript
// SÃºÄasnÃ½ kÃ³d (riadok 403-521)
for (const snapshot of snapshots) {
  // ... processing ...
  await upsertToDB(...);  // â† N queries pre N tickerov
  await prisma.ticker.findUnique(...);  // â† ÄalÅ¡Ã­ch N queries
  await updateRankIndexes(...);  // â† ÄalÅ¡Ã­ch N Redis operÃ¡ciÃ­
}
```

**KonkrÃ©tne merania:**

- **DB writes per batch:** ~150 queries (70 tickerov Ã— 2-3 queries na ticker)
  - `prisma.ticker.upsert()` = 1 query
  - `prisma.sessionPrice.findUnique()` + `upsert()` = 1-2 queries
  - `prisma.dailyRef.upsert()` = 1 query (ak existuje previousClose)
  - `prisma.ticker.findUnique()` pre rank indexes = 1 query
- **Redis ops per batch:** ~150 ops (70 tickerov Ã— 2-3 ops na ticker)
  - `atomicUpdatePrice()` = 1-2 ops
  - `updateRankIndexes()` = 4-6 ops (pre kaÅ¾dÃ½ rank field)
  - `publishTick()` = 1 op
- **Polygon snapshots:** ~3-4 req/min (free tier: 5 req/min, batch size 60-70)
- **Full cycle for 200 tickers:** ~55-70 sec
  - Top 200: 3 batchy Ã— 15s delay = 45s
  - Processing time: ~10-15s per batch Ã— 3 = 30-45s
  - Celkom: ~55-70s

**DÃ´sledok:**

- âš ï¸ **NajvÃ¤ÄÅ¡Ã­ bottleneck:** N+1 queries a Redis operÃ¡cie
- PomalÃ© batch processing (15-20s pre 70 tickerov)
- VysokÃ¡ latencia medzi updates
- NeefektÃ­vne vyuÅ¾itie DB a Redis resources

#### **B. NeoptimalizovanÃ© batch operÃ¡cie**

- KaÅ¾dÃ½ ticker sa spracovÃ¡va individuÃ¡lne
- ChÃ½ba batch upsert do DB (transaction-based)
- ChÃ½ba batch Redis operÃ¡cie (MULTI/EXEC pipeline)

#### **C. Rate limiting**

- FixnÃ½ delay 15s medzi batchmi (neadaptÃ­vny)
- NeadaptÃ­vny k API response times
- NevyuÅ¾Ã­va HTTP/2 multiplexing
- **Polygon API tier limity:**
  - Free: 5 req/min = max 300 tickers/min (batch size 60)
  - Starter ($49): 120 req/min = aÅ¾ 2000 tickers/min
  - Developer ($199): 300 req/min = 5000 tickers/min
  - Scale ($499): 1200 req/min = 20k tickers/min realtime

#### **D. Database queries**

- `getStocksList` pouÅ¾Ã­va `findMany` s limitom, ale mÃ´Å¾e byÅ¥ optimalizovanÃ©
- ChÃ½ba connection pooling (SQLite limit)
- ChÃ½ba query result caching

#### **E. Redis operÃ¡cie**

- KaÅ¾dÃ½ ticker = 1 Redis operÃ¡cia
- ChÃ½ba batch MULTI/EXEC pre viacero tickerov naraz
- ChÃ½ba pipeline pre non-blocking operÃ¡cie

---

## ğŸš€ 2. NavrhovanÃ© vylepÅ¡enia

### 2.1 Batch Database Operations (PRIORITA #1)

#### **A. MigraÄnÃ½ postup - 5-krokovÃ½ checklist**

**Krok 1:** PridaÅ¥ novÃ½ modul `/services/batchDbWriter.ts`

```typescript
// src/services/batchDbWriter.ts
export interface NormalizedSnapshotBatch {
  symbol: string;
  session: MarketSession;
  normalized: ReturnType<typeof normalizeSnapshot>;
  previousClose: number | null;
  marketCap: number;
  marketCapDiff: number;
}

export async function batchUpsertToDB(
  data: NormalizedSnapshotBatch[]
): Promise<boolean[]>;
```

**Krok 2:** ZaviesÅ¥ interface pre `NormalizedSnapshotBatch`

- DefinovaÅ¥ typy v `src/lib/types.ts`
- ExportovaÅ¥ pre pouÅ¾itie v `polygonWorker.ts`

**Krok 3:** Refaktor `polygonWorker.ts` (krok 1: len DB)

- NahradiÅ¥ `for` loop s `upsertToDB()` â†’ `batchUpsertToDB()`
- TestovaÅ¥ len DB batch operÃ¡cie (Redis ponechaÅ¥ pÃ´vodnÃ½)
- ValidovaÅ¥ sprÃ¡vnosÅ¥ dÃ¡t

**Krok 4:** PridaÅ¥ batch Redis pipeline

- VytvoriÅ¥ `batchUpdateRedis()` funkciu
- PouÅ¾iÅ¥ Redis pipeline (MULTI/EXEC)
- IntegrovaÅ¥ s `batchUpsertToDB()`

**Krok 5:** AktivovaÅ¥ batch mode v `polygonWorker` (flag v ENV)

```typescript
// .env
ENABLE_BATCH_MODE = true;

// polygonWorker.ts
const useBatchMode = process.env.ENABLE_BATCH_MODE === "true";
if (useBatchMode) {
  await batchUpsertToDB(batchData);
  await batchUpdateRedis(batchData);
} else {
  // Fallback na pÃ´vodnÃ½ kÃ³d
}
```

#### **B. Batch Upsert do DB - implementÃ¡cia**

```typescript
// NavrhovanÃ¡ implementÃ¡cia
async function batchUpsertToDB(
  data: Array<{
    symbol: string;
    session: MarketSession;
    normalized: ReturnType<typeof normalizeSnapshot>;
    previousClose: number | null;
    marketCap: number;
    marketCapDiff: number;
  }>
): Promise<boolean[]> {
  // PouÅ¾iÅ¥ Prisma transaction s batch operÃ¡ciami
  return await prisma.$transaction(async (tx) => {
    // 1. Batch upsert Ticker
    const tickerUpdates = data.map((d) => ({
      where: { symbol: d.symbol },
      update: {
        lastPrice: d.normalized.price,
        lastChangePct: d.normalized.changePct,
        lastMarketCap: d.marketCap,
        lastMarketCapDiff: d.marketCapDiff,
        lastPriceUpdated: d.normalized.timestamp,
        updatedAt: new Date(),
      },
      create: {
        symbol: d.symbol,
        lastPrice: d.normalized.price,
        // ... ostatnÃ© polia
      },
    }));

    // PouÅ¾iÅ¥ createMany alebo upsertMany (ak Prisma podporuje)
    // Alebo paralelnÃ© Promise.all pre upsert operÃ¡cie

    // 2. Batch upsert SessionPrice
    const sessionPriceUpdates = data.map((d) => ({
      where: {
        symbol_date_session: {
          symbol: d.symbol,
          date: today,
          session: d.session,
        },
      },
      update: {
        /* ... */
      },
      create: {
        /* ... */
      },
    }));

    // 3. Batch upsert DailyRef
    const dailyRefUpdates = data
      .filter((d) => d.previousClose)
      .map((d) => ({
        where: { symbol_date: { symbol: d.symbol, date: today } },
        update: { previousClose: d.previousClose },
        create: {
          symbol: d.symbol,
          date: today,
          previousClose: d.previousClose,
        },
      }));

    // VykonaÅ¥ vÅ¡etky operÃ¡cie paralelne
    await Promise.all([
      ...tickerUpdates.map((u) => tx.ticker.upsert(u)),
      ...sessionPriceUpdates.map((u) => tx.sessionPrice.upsert(u)),
      ...dailyRefUpdates.map((u) => tx.dailyRef.upsert(u)),
    ]);

    return data.map(() => true);
  });
}
```

**OÄakÃ¡vanÃ½ vÃ½kon:**

- Z 70 queries â†’ 1 transaction s batch operÃ¡ciami
- **ZlepÅ¡enie:** 10-20x rÃ½chlejÅ¡ie batch processing

#### **B. Batch Fetch Shares Outstanding**

```typescript
// SÃºÄasnÃ½ kÃ³d uÅ¾ mÃ¡ batch fetch (riadok 374-400), ale mÃ´Å¾e byÅ¥ optimalizovanÃ½
// NavrhovanÃ©: Cache sharesOutstanding v DB a aktualizovaÅ¥ len raz denne
```

### 2.2 Batch Redis Operations

#### **A. Redis Pipeline pre batch operÃ¡cie**

```typescript
async function batchUpdateRedis(
  data: Array<{
    symbol: string;
    session: MarketSession;
    priceData: PriceData;
    marketCap: number;
    marketCapDiff: number;
    changePct: number;
  }>
): Promise<void> {
  const pipeline = redisClient.pipeline();

  data.forEach((d) => {
    // Atomic update pre kaÅ¾dÃ½ ticker
    const lastKey = REDIS_KEYS.lastWithDate(date, d.session, d.symbol);
    pipeline.setEx(
      lastKey,
      ttl,
      JSON.stringify({
        p: d.priceData.p,
        change_pct: d.changePct,
        cap: d.marketCap,
        cap_diff: d.marketCapDiff,
      })
    );

    // Update rank indexes
    pipeline.zAdd(getRankKey("chg", date, d.session) + ":desc", {
      score: -Math.round(d.changePct * 10000),
      value: d.symbol,
    });
    // ... ostatnÃ© rank indexy
  });

  // Execute vÅ¡etky operÃ¡cie naraz
  await pipeline.exec();
}
```

**OÄakÃ¡vanÃ½ vÃ½kon:**

- Z 70 Redis operÃ¡ciÃ­ â†’ 1 pipeline exec
- **ZlepÅ¡enie:** 5-10x rÃ½chlejÅ¡ie Redis updates

### 2.3 Adaptive Rate Limiting (PRIORITA #3)

#### **A. Polygon API Tier Comparison**

| Tier          | Rate Limit   | Cena | ReÃ¡lny dopad na PMP      | Batch Size | Cycle Time (200 tickers) |
| ------------- | ------------ | ---- | ------------------------ | ---------- | ------------------------ |
| **Free**      | 5 req/min    | $0   | max 300 tickers/min      | 60         | ~55-70s                  |
| **Starter**   | 120 req/min  | $49  | aÅ¾ 2000 tickers/min      | 60         | ~10-15s                  |
| **Developer** | 300 req/min  | $199 | 5000 tickers/min         | 70         | ~5-8s                    |
| **Scale**     | 1200 req/min | $499 | 20k tickers/min realtime | 100        | ~2-3s                    |

**OdporÃºÄanie:** Pre produkciu zvÃ¡Å¾iÅ¥ **Starter tier ($49/mesiac)** - 24Ã— zrÃ½chlenie oproti free tieru.

#### **B. Dynamic Batch Size**

```typescript
class AdaptiveRateLimiter {
  private requestCount = 0;
  private windowStart = Date.now();
  private readonly WINDOW_MS = 60000; // 1 minuta
  private readonly MAX_REQUESTS = 250; // Conservative limit

  getOptimalBatchSize(): number {
    const elapsed = Date.now() - this.windowStart;
    if (elapsed > this.WINDOW_MS) {
      this.requestCount = 0;
      this.windowStart = Date.now();
    }

    const remainingRequests = this.MAX_REQUESTS - this.requestCount;
    return Math.min(70, remainingRequests); // Max 70 per batch
  }

  async waitIfNeeded(): Promise<void> {
    if (this.requestCount >= this.MAX_REQUESTS) {
      const waitTime = this.WINDOW_MS - (Date.now() - this.windowStart);
      if (waitTime > 0) {
        await new Promise((resolve) => setTimeout(resolve, waitTime));
        this.requestCount = 0;
        this.windowStart = Date.now();
      }
    }
  }

  recordRequest(): void {
    this.requestCount++;
  }
}
```

#### **B. HTTP/2 Multiplexing**

```typescript
// PouÅ¾iÅ¥ HTTP/2 client pre paralelnÃ© requesty
import { ClientHttp2Session } from "http2";

async function fetchPolygonSnapshotHTTP2(
  tickers: string[],
  apiKey: string
): Promise<PolygonSnapshot[]> {
  // VytvoriÅ¥ HTTP/2 session
  const session = http2.connect("https://api.polygon.io");

  // VyslaÅ¥ vÅ¡etky requesty paralelne (HTTP/2 multiplexing)
  const promises = tickers.map((ticker) => {
    return new Promise<PolygonSnapshot>((resolve, reject) => {
      const req = session.request({
        ":path": `/v2/snapshot/locale/us/markets/stocks/tickers?tickers=${ticker}&apiKey=${apiKey}`,
        ":method": "GET",
      });

      let data = "";
      req.on("data", (chunk) => {
        data += chunk;
      });
      req.on("end", () => {
        resolve(JSON.parse(data));
      });
      req.on("error", reject);
    });
  });

  return Promise.all(promises);
}
```

**OÄakÃ¡vanÃ½ vÃ½kon:**

- Z 15s delay â†’ adaptÃ­vny delay podÄ¾a rate limitu
- **ZlepÅ¡enie:** 2-3x rÃ½chlejÅ¡ie data ingestion

### 2.4 Database Query Optimization

#### **A. Connection Pooling (ak migrujeme na PostgreSQL)**

```typescript
// Prisma schema
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
  // Connection pooling
  connection_limit = 10
}
```

#### **B. Query Result Caching**

```typescript
// Cache Äasto pouÅ¾Ã­vanÃ© queries
const queryCache = new Map<string, { data: any; timestamp: number }>();

async function getStocksListCached(options: {
  limit?: number;
  sort?: string;
  order?: "asc" | "desc";
}): Promise<StockServiceResult> {
  const cacheKey = JSON.stringify(options);
  const cached = queryCache.get(cacheKey);

  if (cached && Date.now() - cached.timestamp < 5000) {
    // 5s cache
    return cached.data;
  }

  const result = await getStocksList(options);
  queryCache.set(cacheKey, { data: result, timestamp: Date.now() });
  return result;
}
```

#### **C. Index Optimization**

```prisma
// PridanÃ© indexy uÅ¾ existujÃº, ale mÃ´Å¾eme pridaÅ¥ composite indexy
model Ticker {
  // ...
  @@index([lastMarketCapDiff, lastChangePct]) // Pre kombinovanÃ© sortovanie
  @@index([sector, lastMarketCapDiff]) // Pre sector filtering + sorting
}
```

### 2.5 Redis Optimization

#### **A. Batch MULTI/EXEC pre vÅ¡etky tickery**

```typescript
// SÃºÄasnÃ½ kÃ³d uÅ¾ pouÅ¾Ã­va MULTI/EXEC pre jednotlivÃ© tickery
// NavrhovanÃ©: Batch MULTI/EXEC pre celÃ½ batch tickerov
async function batchUpdateRankIndexes(
  date: string,
  session: "pre" | "live" | "after",
  data: RankIndexData[]
): Promise<void> {
  const multi = redisClient.multi();

  data.forEach((d) => {
    // VÅ¡etky operÃ¡cie pre jeden ticker
    const lastKey = REDIS_KEYS.lastWithDate(date, session, d.symbol);
    multi.setEx(
      lastKey,
      ttl,
      JSON.stringify({
        /* ... */
      })
    );

    // Rank indexy
    multi.zAdd(getRankKey("chg", date, session) + ":desc", {
      score: -Math.round(d.changePct * 10000),
      value: d.symbol,
    });
    // ... ostatnÃ© indexy
  });

  // Execute vÅ¡etko naraz
  await multi.exec();
}
```

#### **B. Redis Pipeline pre non-blocking operÃ¡cie**

```typescript
// PouÅ¾iÅ¥ pipeline namiesto MULTI/EXEC pre read operÃ¡cie
const pipeline = redisClient.pipeline();
data.forEach((d) => {
  pipeline.get(`last:${date}:${session}:${d.symbol}`);
});
const results = await pipeline.exec();
```

### 2.6 Worker Prioritization Enhancement

#### **A. Dynamic Priority Queue**

```typescript
interface PriorityTicker {
  symbol: string;
  priority: number; // 1-10, vyÅ¡Å¡ie = dÃ´leÅ¾itejÅ¡ie
  lastUpdate: number;
  updateInterval: number; // ms
}

class PriorityQueue {
  private queue: PriorityTicker[] = [];

  add(ticker: PriorityTicker): void {
    this.queue.push(ticker);
    this.queue.sort((a, b) => b.priority - a.priority);
  }

  getNextBatch(size: number): string[] {
    const now = Date.now();
    const ready = this.queue.filter(
      (t) => now - t.lastUpdate >= t.updateInterval
    );
    return ready.slice(0, size).map((t) => t.symbol);
  }
}
```

### 2.7 Monitoring & Observability

#### **A. Metrics Collection**

```typescript
interface WorkerMetrics {
  batchSize: number;
  processingTime: number;
  dbQueries: number;
  redisOps: number;
  apiCalls: number;
  errors: number;
}

class MetricsCollector {
  private metrics: WorkerMetrics[] = [];

  recordBatch(metrics: WorkerMetrics): void {
    this.metrics.push(metrics);
    // Log alebo export do monitoring systÃ©mu
  }

  getAverageProcessingTime(): number {
    return (
      this.metrics.reduce((sum, m) => sum + m.processingTime, 0) /
      this.metrics.length
    );
  }
}
```

#### **B. Error Tracking**

```typescript
// PouÅ¾iÅ¥ structured logging
import { createLogger } from 'winston';

const logger = createLogger({
  format: winston.format.json(),
  transports: [
    new winston.transports.File({ filename: 'worker-error.log' }),
    new winston.transports.Console()
  ]
});

// Log errors s kontextom
logger.error('Batch processing failed', {
  batchSize: 70,
  tickers: ['AAPL', 'MSFT', ...],
  error: error.message,
  stack: error.stack
});
```

---

## ğŸ“ˆ 3. OÄakÃ¡vanÃ© zlepÅ¡enia vÃ½konu

### 3.1 Batch Processing

- **SÃºÄasnÃ½ Äas:** ~15-20s pre 70 tickerov
- **Po optimalizÃ¡cii:** ~2-3s pre 70 tickerov
- **ZlepÅ¡enie:** **5-7Ã— rÃ½chlejÅ¡ie**

### 3.2 Database Queries

- **SÃºÄasnÃ½ poÄet:** ~150 queries per batch (70 tickerov)
- **Po optimalizÃ¡cii:** 1-3 queries per batch (transaction)
- **ZlepÅ¡enie:** **50-150Ã— menej queries** (z ~150 na 1-3)

### 3.3 Redis Operations

- **SÃºÄasnÃ½ poÄet:** ~150 ops per batch (70 tickerov)
- **Po optimalizÃ¡cii:** 1 pipeline exec per batch
- **ZlepÅ¡enie:** **150Ã— menej operÃ¡ciÃ­** (z ~150 na 1)

### 3.4 API Rate Limiting

- **SÃºÄasnÃ½ delay:** 15s fixnÃ½ (free tier)
- **Po optimalizÃ¡cii:** AdaptÃ­vny (0-15s podÄ¾a rate limitu)
- **S Polygon Starter tier:** 0.5s delay â†’ **30Ã— rÃ½chlejÅ¡ie**
- **ZlepÅ¡enie:** **2-3Ã— rÃ½chlejÅ¡ie ingestion** (free tier) alebo **30Ã—** (Starter tier)

### 3.5 CelkovÃ© zlepÅ¡enie

- **SÃºÄasnÃ½ cyklus:** ~55-70s pre top 200 tickerov (free tier)
- **Po optimalizÃ¡cii (free tier):** ~10-15s pre top 200 tickerov
- **Po optimalizÃ¡cii (Starter tier):** ~2-3s pre top 200 tickerov
- **ZlepÅ¡enie:** **4-6Ã— rÃ½chlejÅ¡ie** (free tier) alebo **20-35Ã—** (Starter tier)

---

## ğŸ¯ 4. Priorita implementÃ¡cie

### **ğŸ”¥ VysokÃ¡ priorita (okamÅ¾itÃ© zlepÅ¡enie)**

**NajvÃ¤ÄÅ¡Ã­ bottleneck:** N+1 queries (~150 DB queries + ~150 Redis ops per batch)

1. âœ… **Batch Database Operations** - **NAJVÃ„ÄŒÅ Ã IMPACT**

   - ZnÃ­Å¾enie z ~150 queries na 1-3 queries per batch
   - **50-150Ã— zlepÅ¡enie** DB performance
   - ImplementÃ¡cia: 1-2 dni

2. âœ… **Batch Redis Operations** - vÃ½raznÃ© zrÃ½chlenie

   - ZnÃ­Å¾enie z ~150 ops na 1 pipeline exec per batch
   - **150Ã— zlepÅ¡enie** Redis performance
   - ImplementÃ¡cia: 1 deÅˆ

3. âœ… **Adaptive Rate Limiting** - lepÅ¡ie vyuÅ¾itie API limitu
   - ZnÃ­Å¾enie fixnÃ©ho 15s delay na adaptÃ­vny (0-15s)
   - **2-3Ã— zlepÅ¡enie** ingestion speed (free tier)
   - **30Ã— zlepÅ¡enie** s Polygon Starter tier ($49/mesiac)
   - ImplementÃ¡cia: 1 deÅˆ

### **StrednÃ¡ priorita (postupnÃ© zlepÅ¡enie)**

4. âš ï¸ **Query Result Caching** - znÃ­Å¾enie DB load
5. âš ï¸ **HTTP/2 Multiplexing** - rÃ½chlejÅ¡ie API calls
6. âš ï¸ **Monitoring & Metrics** - lepÅ¡ia observability

### **NÃ­zka priorita (dlhodobÃ© vylepÅ¡enie)**

7. â„¹ï¸ **Connection Pooling** - ak migrujeme na PostgreSQL
8. â„¹ï¸ **Dynamic Priority Queue** - pokroÄilÃ¡ prioritizÃ¡cia
9. â„¹ï¸ **Error Tracking** - structured logging

---

## ğŸ“ 5. ImplementaÄnÃ© poznÃ¡mky

### 5.1 Breaking Changes

- Å½iadne breaking changes - vÅ¡etky zmeny sÃº backward compatible

### 5.2 Testing

- Unit testy pre batch operÃ¡cie
- Integration testy pre worker cyklus
- Performance testy pre meranie zlepÅ¡enia

### 5.3 Rollout Strategy (5-krokovÃ½ postup)

**FÃ¡za 1:** Batch DB operations (1-2 dni)

- ImplementovaÅ¥ `batchDbWriter.ts`
- Refaktor `polygonWorker.ts` (len DB, Redis ponechaÅ¥)
- TestovaÅ¥ sprÃ¡vnosÅ¥ dÃ¡t

**FÃ¡za 2:** Batch Redis operations (1 deÅˆ)

- ImplementovaÅ¥ `batchUpdateRedis()`
- IntegrovaÅ¥ s batch DB operations
- TestovaÅ¥ Redis pipeline

**FÃ¡za 3:** Adaptive rate limiting (1 deÅˆ)

- ImplementovaÅ¥ `AdaptiveRateLimiter`
- IntegrovaÅ¥ s batch operations
- TestovaÅ¥ rÃ´zne rate limit scenÃ¡re

**FÃ¡za 4:** Monitoring & metrics (1 deÅˆ)

- PridaÅ¥ metrics collection
- ImplementovaÅ¥ structured logging
- Dashboard pre monitoring

**FÃ¡za 5:** Production rollout

- AktivovaÅ¥ `ENABLE_BATCH_MODE=true` v staging
- MonitorovaÅ¥ vÃ½kon 24-48h
- Rollout do produkcie s feature flag

---

## ğŸ”— 6. Referencie

- **Prisma Batch Operations:** https://www.prisma.io/docs/concepts/components/prisma-client/transactions
- **Redis Pipeline:** https://redis.io/docs/manual/pipelining/
- **HTTP/2 Multiplexing:** https://http2.github.io/
- **Polygon API Rate Limits:** https://polygon.io/docs/getting-started

---

**Kontakt:** Pre otÃ¡zky alebo diskusiu o implementÃ¡cii kontaktujte vÃ½vojovÃ½ tÃ­m.
